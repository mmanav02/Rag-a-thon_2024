{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9612916,"sourceType":"datasetVersion","datasetId":5865925}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nest_asyncio\n\nnest_asyncio.apply()","metadata":{"id":"L9sDSuocmarA","execution":{"iopub.status.busy":"2024-10-13T18:43:12.715415Z","iopub.execute_input":"2024-10-13T18:43:12.715905Z","iopub.status.idle":"2024-10-13T18:43:12.723478Z","shell.execute_reply.started":"2024-10-13T18:43:12.715867Z","shell.execute_reply":"2024-10-13T18:43:12.722278Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install -U llama-index llama-index-utils-workflow","metadata":{"id":"uhBEvUDlqkaP","execution":{"iopub.status.busy":"2024-10-13T18:43:12.865474Z","iopub.execute_input":"2024-10-13T18:43:12.865924Z","iopub.status.idle":"2024-10-13T18:43:39.017169Z","shell.execute_reply.started":"2024-10-13T18:43:12.865881Z","shell.execute_reply":"2024-10-13T18:43:39.015585Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting llama-index\n  Downloading llama_index-0.11.17-py3-none-any.whl.metadata (11 kB)\nCollecting llama-index-utils-workflow\n  Downloading llama_index_utils_workflow-0.2.1-py3-none-any.whl.metadata (667 bytes)\nCollecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\nCollecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n  Downloading llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting llama-index-core<0.12.0,>=0.11.17 (from llama-index)\n  Downloading llama_index_core-0.11.17-py3-none-any.whl.metadata (2.4 kB)\nCollecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\nCollecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n  Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\nCollecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index)\n  Downloading llama_index_llms_openai-0.2.13-py3-none-any.whl.metadata (3.3 kB)\nCollecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n  Downloading llama_index_multi_modal_llms_openai-0.2.2-py3-none-any.whl.metadata (678 bytes)\nCollecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\nCollecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\nCollecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n  Downloading llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\nCollecting nltk>3.8.1 (from llama-index)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting pyvis<0.4.0,>=0.3.2 (from llama-index-utils-workflow)\n  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\nCollecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (6.0.2)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.17->llama-index) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (3.9.5)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.2.14)\nCollecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.17->llama-index)\n  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (2024.6.1)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (0.27.0)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.6.0)\nRequirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (3.3)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.26.4)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (10.3.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (2.9.2)\nRequirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (8.3.0)\nCollecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.17->llama-index)\n  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (4.12.2)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.17->llama-index) (1.16.0)\nCollecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n  Downloading llama_cloud-0.1.2-py3-none-any.whl.metadata (763 bytes)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.3)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\nCollecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\nCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\nCollecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index)\n  Downloading llama_parse-0.5.7-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (2024.5.15)\nRequirement already satisfied: ipython>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (8.21.0)\nRequirement already satisfied: jinja2>=2.9.6 in /opt/conda/lib/python3.10/site-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.1.4)\nCollecting jsonpickle>=1.4.1 (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow)\n  Downloading jsonpickle-3.3.0-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.17->llama-index) (4.0.3)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.5)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.1.7)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (3.0.47)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.18.0)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.6.2)\nRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (5.14.3)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (4.9.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.9.6->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.1.5)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.17->llama-index) (4.4.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.17->llama-index) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.17->llama-index) (3.7)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.17->llama-index) (0.14.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\nCollecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.17->llama-index) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.17->llama-index) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.17->llama-index) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.26.18)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.17->llama-index) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.17->llama-index) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.17->llama-index) (3.22.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.8.4)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.17->llama-index) (21.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.13)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow) (0.2.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.17->llama-index) (3.1.2)\nDownloading llama_index-0.11.17-py3-none-any.whl (6.8 kB)\nDownloading llama_index_utils_workflow-0.2.1-py3-none-any.whl (2.6 kB)\nDownloading llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\nDownloading llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\nDownloading llama_index_core-0.11.17-py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\nDownloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl (10 kB)\nDownloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_index_llms_openai-0.2.13-py3-none-any.whl (13 kB)\nDownloading llama_index_multi_modal_llms_openai-0.2.2-py3-none-any.whl (5.9 kB)\nDownloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\nDownloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\nDownloading llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\nDownloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\nDownloading jsonpickle-3.3.0-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_cloud-0.1.2-py3-none-any.whl (173 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.8/173.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_parse-0.5.7-py3-none-any.whl (10 kB)\nDownloading openai-1.51.2-py3-none-any.whl (383 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\nDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: striprtf, dirtyjson, pypdf, nltk, jsonpickle, jiter, tiktoken, openai, llama-cloud, pyvis, llama-index-legacy, llama-index-core, llama-parse, llama-index-utils-workflow, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n  Attempting uninstall: pypdf\n    Found existing installation: pypdf 5.0.1\n    Uninstalling pypdf-5.0.1:\n      Successfully uninstalled pypdf-5.0.1\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dirtyjson-1.0.8 jiter-0.6.1 jsonpickle-3.3.0 llama-cloud-0.1.2 llama-index-0.11.17 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.17 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.4.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.13 llama-index-multi-modal-llms-openai-0.2.2 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-index-utils-workflow-0.2.1 llama-parse-0.5.7 nltk-3.9.1 openai-1.51.2 pypdf-4.3.1 pyvis-0.3.2 striprtf-0.0.26 tiktoken-0.8.0\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install -U pinecone llama-index-vector-stores-pinecone","metadata":{"id":"zSpfS6J6wR59","execution":{"iopub.status.busy":"2024-10-13T18:43:39.020165Z","iopub.execute_input":"2024-10-13T18:43:39.020808Z","iopub.status.idle":"2024-10-13T18:43:57.303950Z","shell.execute_reply.started":"2024-10-13T18:43:39.020646Z","shell.execute_reply":"2024-10-13T18:43:57.301795Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting pinecone\n  Downloading pinecone-5.3.1-py3-none-any.whl.metadata (19 kB)\nCollecting llama-index-vector-stores-pinecone\n  Downloading llama_index_vector_stores_pinecone-0.2.1-py3-none-any.whl.metadata (667 bytes)\nRequirement already satisfied: certifi>=2019.11.17 in /opt/conda/lib/python3.10/site-packages (from pinecone) (2024.8.30)\nCollecting pinecone-plugin-inference<2.0.0,>=1.1.0 (from pinecone)\n  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\nCollecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from pinecone) (2.9.0.post0)\nRequirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from pinecone) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from pinecone) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from pinecone) (1.26.18)\nRequirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-vector-stores-pinecone) (0.11.17)\nCollecting pinecone-client<6.0.0,>=3.2.2 (from llama-index-vector-stores-pinecone)\n  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (6.0.2)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (3.9.5)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.2.14)\nRequirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.0.8)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (2024.6.1)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (0.27.0)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.6.0)\nRequirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (3.3)\nRequirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (3.9.1)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.26.4)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (10.3.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (2.9.2)\nRequirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (8.3.0)\nRequirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (0.8.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.16.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone) (1.16.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (4.0.3)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (2024.5.15)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (3.7)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (3.22.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (0.14.0)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (21.3)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-vector-stores-pinecone) (3.1.2)\nDownloading pinecone-5.3.1-py3-none-any.whl (419 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/419.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading llama_index_vector_stores_pinecone-0.2.1-py3-none-any.whl (7.0 kB)\nDownloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\nInstalling collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client, pinecone, llama-index-vector-stores-pinecone\nSuccessfully installed llama-index-vector-stores-pinecone-0.2.1 pinecone-5.3.1 pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install -U llama-index-embeddings-mistralai llama-index-llms-text-generation-inference","metadata":{"id":"-59x6rrl4AM9","execution":{"iopub.status.busy":"2024-10-13T18:43:57.306388Z","iopub.execute_input":"2024-10-13T18:43:57.306889Z","iopub.status.idle":"2024-10-13T18:44:12.280437Z","shell.execute_reply.started":"2024-10-13T18:43:57.306837Z","shell.execute_reply":"2024-10-13T18:44:12.278870Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting llama-index-embeddings-mistralai\n  Downloading llama_index_embeddings_mistralai-0.2.0-py3-none-any.whl.metadata (696 bytes)\nCollecting llama-index-llms-text-generation-inference\n  Downloading llama_index_llms_text_generation_inference-0.2.2-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-embeddings-mistralai) (0.11.17)\nCollecting mistralai>=1.0.0 (from llama-index-embeddings-mistralai)\n  Downloading mistralai-1.1.0-py3-none-any.whl.metadata (23 kB)\nCollecting llama-index-utils-huggingface<0.3.0,>=0.2.0 (from llama-index-llms-text-generation-inference)\n  Downloading llama_index_utils_huggingface-0.2.0-py3-none-any.whl.metadata (699 bytes)\nCollecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-text-generation-inference)\n  Downloading text_generation-0.7.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (6.0.2)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (3.9.5)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.2.14)\nRequirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.0.8)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (2024.6.1)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (0.27.0)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.6.0)\nRequirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (3.3)\nRequirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (3.9.1)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.26.4)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (10.3.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (2.9.2)\nRequirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (8.3.0)\nRequirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (0.8.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (4.12.2)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.16.0)\nRequirement already satisfied: huggingface-hub>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-utils-huggingface<0.3.0,>=0.2.0->llama-index-llms-text-generation-inference) (0.25.1)\nRequirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from mistralai>=1.0.0->llama-index-embeddings-mistralai) (0.2.0)\nCollecting jsonpath-python<2.0.0,>=1.0.6 (from mistralai>=1.0.0->llama-index-embeddings-mistralai)\n  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\nCollecting python-dateutil==2.8.2 (from mistralai>=1.0.0->llama-index-embeddings-mistralai)\n  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil==2.8.2->mistralai>=1.0.0->llama-index-embeddings-mistralai) (1.16.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (4.0.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (4.4.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (3.7)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-utils-huggingface<0.3.0,>=0.2.0->llama-index-llms-text-generation-inference) (3.15.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-utils-huggingface<0.3.0,>=0.2.0->llama-index-llms-text-generation-inference) (21.3)\nCollecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-utils-huggingface<0.3.0,>=0.2.0->llama-index-llms-text-generation-inference)\n  Downloading minijinja-2.2.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (2024.5.15)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.26.18)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (3.22.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-utils-huggingface<0.3.0,>=0.2.0->llama-index-llms-text-generation-inference) (3.1.2)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-mistralai) (1.2.0)\nDownloading llama_index_embeddings_mistralai-0.2.0-py3-none-any.whl (2.6 kB)\nDownloading llama_index_llms_text_generation_inference-0.2.2-py3-none-any.whl (6.5 kB)\nDownloading llama_index_utils_huggingface-0.2.0-py3-none-any.whl (2.9 kB)\nDownloading mistralai-1.1.0-py3-none-any.whl (229 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading text_generation-0.7.0-py3-none-any.whl (12 kB)\nDownloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\nDownloading minijinja-2.2.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (861 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m861.9/861.9 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: python-dateutil, minijinja, jsonpath-python, text-generation, mistralai, llama-index-utils-huggingface, llama-index-embeddings-mistralai, llama-index-llms-text-generation-inference\n  Attempting uninstall: python-dateutil\n    Found existing installation: python-dateutil 2.9.0.post0\n    Uninstalling python-dateutil-2.9.0.post0:\n      Successfully uninstalled python-dateutil-2.9.0.post0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 17.0.0 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nxarray 2024.9.0 requires packaging>=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed jsonpath-python-1.0.6 llama-index-embeddings-mistralai-0.2.0 llama-index-llms-text-generation-inference-0.2.2 llama-index-utils-huggingface-0.2.0 minijinja-2.2.0 mistralai-1.1.0 python-dateutil-2.8.2 text-generation-0.7.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install -U llama-index-core llama-parse llama-index-readers-file python-dotenv","metadata":{"id":"1iaSn7LbKEiu","execution":{"iopub.status.busy":"2024-10-13T18:44:12.284575Z","iopub.execute_input":"2024-10-13T18:44:12.285060Z","iopub.status.idle":"2024-10-13T18:44:25.261081Z","shell.execute_reply.started":"2024-10-13T18:44:12.285014Z","shell.execute_reply":"2024-10-13T18:44:25.259217Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: llama-index-core in /opt/conda/lib/python3.10/site-packages (0.11.17)\nRequirement already satisfied: llama-parse in /opt/conda/lib/python3.10/site-packages (0.5.7)\nRequirement already satisfied: llama-index-readers-file in /opt/conda/lib/python3.10/site-packages (0.2.2)\nRequirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (1.0.1)\nRequirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (6.0.2)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (3.9.5)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (1.2.14)\nRequirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (1.0.8)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (2024.6.1)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (0.27.0)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (1.6.0)\nRequirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (3.3)\nRequirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (3.9.1)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (1.26.4)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (10.3.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (2.9.2)\nRequirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (8.3.0)\nRequirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (0.8.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (4.12.2)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core) (1.16.0)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file) (4.12.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file) (2.2.3)\nRequirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file) (4.3.1)\nRequirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file) (0.0.26)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (4.0.3)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.5)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core) (2024.5.15)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core) (3.22.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-readers-file) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-readers-file) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-readers-file) (2024.1)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (21.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.16.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (3.1.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install -U llama-index-readers-file","metadata":{"id":"GXtQc6r5lrJk","execution":{"iopub.status.busy":"2024-10-13T18:44:25.263493Z","iopub.execute_input":"2024-10-13T18:44:25.264147Z","iopub.status.idle":"2024-10-13T18:44:37.931135Z","shell.execute_reply.started":"2024-10-13T18:44:25.264085Z","shell.execute_reply":"2024-10-13T18:44:37.929251Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: llama-index-readers-file in /opt/conda/lib/python3.10/site-packages (0.2.2)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file) (4.12.3)\nRequirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file) (0.11.17)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file) (2.2.3)\nRequirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file) (4.3.1)\nRequirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file) (0.0.26)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.5)\nRequirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (6.0.2)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.9.5)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.2.14)\nRequirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.0.8)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2024.6.1)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (0.27.0)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.6.0)\nRequirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.3)\nRequirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.9.1)\nRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.26.4)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (10.3.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2.9.2)\nRequirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (8.3.0)\nRequirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (0.8.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (4.12.2)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-readers-file) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-readers-file) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-readers-file) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (4.0.3)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2024.5.15)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2.23.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.22.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (0.14.0)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (21.3)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-readers-file) (3.1.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install -U datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FqkYnFCsdg47","outputId":"93aa7ead-bedc-4c31-d904-87438ef21c69","execution":{"iopub.status.busy":"2024-10-13T18:44:37.933215Z","iopub.execute_input":"2024-10-13T18:44:37.933662Z","iopub.status.idle":"2024-10-13T18:44:50.676620Z","shell.execute_reply.started":"2024-10-13T18:44:37.933618Z","shell.execute_reply":"2024-10-13T18:44:50.674736Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"API KEYS","metadata":{}},{"cell_type":"code","source":"import os\nimport getpass\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nos.environ[\"PINECONE_API_KEY\"] = user_secrets.get_secret(\"PINECONE_API_KEY\")\nos.environ[\"LLAMA_CLOUD_API_KEY\"] = user_secrets.get_secret(\"LLAMA_CLOUD_API_KEY\")\nos.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")\nos.environ[\"MISTRAL_API_KEY\"] = user_secrets.get_secret(\"MISTRAL_API_KEY\")\nos.environ['OPENAI_API_KEY'] = user_secrets.get_secret(\"OPENAI_API_KEY\")","metadata":{"id":"XtN9SIIKwdkg","execution":{"iopub.status.busy":"2024-10-13T18:44:50.678703Z","iopub.execute_input":"2024-10-13T18:44:50.679149Z","iopub.status.idle":"2024-10-13T18:44:51.122298Z","shell.execute_reply.started":"2024-10-13T18:44:50.679093Z","shell.execute_reply":"2024-10-13T18:44:51.120809Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n# please install HuggingFace datasets by pip install datasets\n\nmulti_lexsum = load_dataset(\"allenai/multi_lexsum\", name=\"v20220616\", trust_remote_code=True)","metadata":{"id":"_HsatPZ2Ig5Q","execution":{"iopub.status.busy":"2024-10-13T18:44:51.123758Z","iopub.execute_input":"2024-10-13T18:44:51.124296Z","iopub.status.idle":"2024-10-13T18:44:51.692958Z","shell.execute_reply.started":"2024-10-13T18:44:51.124220Z","shell.execute_reply":"2024-10-13T18:44:51.691418Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from llama_index.core import Document\nfrom llama_index.core.node_parser import TokenTextSplitter\nfrom tqdm import tqdm\n\ndoc_list = []\ntoken_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=50)\nfor i in tqdm(range(0, len(multi_lexsum['train']['sources']))):\n    chunks = token_splitter.split_text(str(multi_lexsum['train']['sources'][i]))\n    for chunk in chunks:\n        doc_list.append(Document(text=chunk, metadata = {'id': str(multi_lexsum['train']['id'][i])}))","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:44:51.694712Z","iopub.execute_input":"2024-10-13T18:44:51.695121Z","iopub.status.idle":"2024-10-13T21:39:42.366567Z","shell.execute_reply.started":"2024-10-13T18:44:51.695081Z","shell.execute_reply":"2024-10-13T21:39:42.365069Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 3177/3177 [2:54:43<00:00,  3.30s/it]  \n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/working/doc_list_longsummary.pkl', 'wb') as f:  # open a text file\n    pickle.dump(doc_list, f)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:24:10.406011Z","iopub.execute_input":"2024-10-13T18:24:10.406528Z","iopub.status.idle":"2024-10-13T18:24:10.550973Z","shell.execute_reply.started":"2024-10-13T18:24:10.406482Z","shell.execute_reply":"2024-10-13T18:24:10.549817Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('/kaggle/working/doc_list_longsummary.pkl', 'rb') as f:\n    doc_list = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:24:14.488177Z","iopub.execute_input":"2024-10-13T18:24:14.488682Z","iopub.status.idle":"2024-10-13T18:24:14.868491Z","shell.execute_reply.started":"2024-10-13T18:24:14.488636Z","shell.execute_reply":"2024-10-13T18:24:14.867247Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!pip install -U langchain-text-splitters","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:24:15.647532Z","iopub.execute_input":"2024-10-13T18:24:15.647996Z","iopub.status.idle":"2024-10-13T18:24:32.719622Z","shell.execute_reply.started":"2024-10-13T18:24:15.647953Z","shell.execute_reply":"2024-10-13T18:24:32.718220Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Collecting langchain-text-splitters\n  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting langchain-core<0.4.0,>=0.3.0 (from langchain-text-splitters)\n  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (6.0.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (1.33)\nCollecting langsmith<0.2.0,>=0.1.125 (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters)\n  Downloading langsmith-0.1.134-py3-none-any.whl.metadata (13 kB)\nCollecting packaging<25,>=23.2 (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (2.9.2)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (8.3.0)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (4.12.2)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (2.4)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (3.10.4)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (2.32.3)\nCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (2.23.4)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (4.4.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (3.7)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (0.14.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (1.26.18)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-text-splitters) (1.2.0)\nDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\nDownloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langsmith-0.1.134-py3-none-any.whl (295 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, requests-toolbelt, langsmith, langchain-core, langchain-text-splitters\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: requests-toolbelt\n    Found existing installation: requests-toolbelt 0.10.1\n    Uninstalling requests-toolbelt-0.10.1:\n      Successfully uninstalled requests-toolbelt-0.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nthinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-core-0.3.10 langchain-text-splitters-0.3.0 langsmith-0.1.134 packaging-24.1 requests-toolbelt-1.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"len(chunks)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:24:32.722945Z","iopub.execute_input":"2024-10-13T18:24:32.723566Z","iopub.status.idle":"2024-10-13T18:24:32.735098Z","shell.execute_reply.started":"2024-10-13T18:24:32.723505Z","shell.execute_reply":"2024-10-13T18:24:32.733829Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"markdown","source":"Next, we'll use our [mistral-embed](https://docs.mistral.ai/capabilities/embeddings/) model as our default Embedding model!","metadata":{"id":"dYTJD52RVxj2"}},{"cell_type":"code","source":"from llama_index.embeddings.mistralai import MistralAIEmbedding\n\nmodel_name=\"mistral-embed\"\n\nembed_model = MistralAIEmbedding(model_name=model_name)","metadata":{"id":"ZASnfwLJNMJb","execution":{"iopub.status.busy":"2024-10-13T18:24:32.737021Z","iopub.execute_input":"2024-10-13T18:24:32.737587Z","iopub.status.idle":"2024-10-13T18:24:33.219888Z","shell.execute_reply.started":"2024-10-13T18:24:32.737529Z","shell.execute_reply":"2024-10-13T18:24:33.218849Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Let's test this out - and grab our embedding dimension by checking the length of the returned response.","metadata":{"id":"mDSN8ZXfWCdc"}},{"cell_type":"code","source":"embeddings = embed_model.get_text_embedding(\"Welcome to the RAGATHON!\")\nprint(len(embeddings))\nembedding_dimension = len(embeddings)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vpfTbOwNOzl2","outputId":"6ee22abf-8f60-4bf0-82af-26a0e6121c32","execution":{"iopub.status.busy":"2024-10-13T18:24:33.222751Z","iopub.execute_input":"2024-10-13T18:24:33.223265Z","iopub.status.idle":"2024-10-13T18:24:33.976653Z","shell.execute_reply.started":"2024-10-13T18:24:33.223208Z","shell.execute_reply":"2024-10-13T18:24:33.975295Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"1024\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's save this model in our settings!","metadata":{"id":"xnJsZrUNWIpY"}},{"cell_type":"code","source":"from llama_index.core import Settings\n\nSettings.embed_model = embed_model","metadata":{"id":"5aVW8hLqS4Pv","execution":{"iopub.status.busy":"2024-10-13T18:24:33.978329Z","iopub.execute_input":"2024-10-13T18:24:33.978744Z","iopub.status.idle":"2024-10-13T18:24:33.986108Z","shell.execute_reply.started":"2024-10-13T18:24:33.978702Z","shell.execute_reply":"2024-10-13T18:24:33.984669Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from pinecone import Pinecone, ServerlessSpec\n\npc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])","metadata":{"id":"qjrVUt_UwZ9m","execution":{"iopub.status.busy":"2024-10-13T18:25:01.639939Z","iopub.execute_input":"2024-10-13T18:25:01.640477Z","iopub.status.idle":"2024-10-13T18:25:01.777983Z","shell.execute_reply.started":"2024-10-13T18:25:01.640430Z","shell.execute_reply":"2024-10-13T18:25:01.776222Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Next, we'll create our Pinecone Index through the Pinecone client!","metadata":{"id":"joS9OEyFW3KY"}},{"cell_type":"code","source":"index_name = \"llamaindex-ragathon-lawllm-l-sum-chunks-index\"\n\npc.create_index(\n    name=index_name,\n    dimension=embedding_dimension,\n    metric=\"cosine\",\n    spec=ServerlessSpec(\n        cloud=\"aws\",\n        region=\"us-east-1\"\n    )\n)","metadata":{"id":"z7KOUdp8wrGM","execution":{"iopub.status.busy":"2024-10-13T18:25:03.623865Z","iopub.execute_input":"2024-10-13T18:25:03.624347Z","iopub.status.idle":"2024-10-13T18:25:09.054736Z","shell.execute_reply.started":"2024-10-13T18:25:03.624302Z","shell.execute_reply":"2024-10-13T18:25:09.053353Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"pinecone_index = pc.Index(index_name)","metadata":{"id":"ekaV8FFwykyn","execution":{"iopub.status.busy":"2024-10-13T18:25:11.261056Z","iopub.execute_input":"2024-10-13T18:25:11.262205Z","iopub.status.idle":"2024-10-13T18:25:11.364179Z","shell.execute_reply.started":"2024-10-13T18:25:11.262144Z","shell.execute_reply":"2024-10-13T18:25:11.363044Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## LLM\n\nWe can point at our inference endpoint (that we set-up already) by including our URL below!","metadata":{"id":"W9zyl6NPW79w"}},{"cell_type":"code","source":"import os\nfrom typing import List, Optional\n\nfrom llama_index.llms.text_generation_inference import (\n    TextGenerationInference,\n)\n\nURL = \"https://l26a2czy10oerq1x.us-east-1.aws.endpoints.huggingface.cloud\"\nhf_llm = TextGenerationInference(\n    model_url=URL, token=os.environ[\"HF_TOKEN\"]\n)\n\ncompletion_response = hf_llm.complete(\"To infinity, and\")\nprint(completion_response)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S27bM5IpPwU5","outputId":"37e7267e-a1be-415a-985e-67ef64efb624","execution":{"iopub.status.busy":"2024-10-13T18:25:16.411813Z","iopub.execute_input":"2024-10-13T18:25:16.412333Z","iopub.status.idle":"2024-10-13T18:25:17.514908Z","shell.execute_reply.started":"2024-10-13T18:25:16.412287Z","shell.execute_reply":"2024-10-13T18:25:17.513627Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n\nYou may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"...beyond!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## RAG Prompt\n\nWe'll set up and provide a classic RAG Prompt, of course!","metadata":{"id":"_8B73qq_XKTL"}},{"cell_type":"code","source":"from llama_index.core import PromptTemplate\n\nDEFAULT_RAG_PROMPT = PromptTemplate(\n    template=\"\"\"You are an AI assistant engaged in a conversation. Use the provided conversation history and context to answer the current question. If you don't know the answer based on the given information, say you don't know.\n\n    Conversation History:\n    {conversation_history}\n\n    Additional Context:\n    {context}\n\n    Current Question:\n    {question}\n\n    Answer the current question, taking into account the conversation history and additional context:\n    \"\"\"\n)","metadata":{"id":"f4I5jTYifgC0","execution":{"iopub.status.busy":"2024-10-13T18:25:18.964622Z","iopub.execute_input":"2024-10-13T18:25:18.965153Z","iopub.status.idle":"2024-10-13T18:25:18.971910Z","shell.execute_reply.started":"2024-10-13T18:25:18.965105Z","shell.execute_reply":"2024-10-13T18:25:18.970637Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from llama_index.core.workflow import Event\nfrom llama_index.core.schema import NodeWithScore\n\nclass PrepEvent(Event):\n    \"\"\"Prep event (prepares for retrieval).\"\"\"\n    pass\n\nclass RetrieveEvent(Event):\n    \"\"\"Retrieve event (gets retrieved nodes).\"\"\"\n\n    retrieved_nodes: list[NodeWithScore]\n\nclass AugmentGenerateEvent(Event):\n    \"\"\"Query event. Queries given relevant text and search text.\"\"\"\n    relevant_text: str\n    search_text: str","metadata":{"id":"_-2DD81xgg77","execution":{"iopub.status.busy":"2024-10-13T18:25:19.245347Z","iopub.execute_input":"2024-10-13T18:25:19.245798Z","iopub.status.idle":"2024-10-13T18:25:19.282992Z","shell.execute_reply.started":"2024-10-13T18:25:19.245756Z","shell.execute_reply":"2024-10-13T18:25:19.281809Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Setting up Steps\n\nNext, we'll define our `Steps`!\n\nRemember: A `Step` must be triggered by one or more `Events`, and it must emit an `Event`.\n\nTo get started with our Workflow, we'll need to define a Workflow class.\n\nLet's do that!","metadata":{"id":"5NB1CYAUXjCb"}},{"cell_type":"markdown","source":"#### An Aside on Context:\n\n`Context`, in workflows, is analagous to `State` in frameworks like LangGraph.\n\nIt's a way to provide information to multiple `Steps`, without needing to constantly carry forward information in each `Event`.","metadata":{"id":"TjDS326mX3yO"}},{"cell_type":"code","source":"from llama_index.core.workflow import (\n    Workflow,\n    step,\n    Context,\n    StartEvent,\n    StopEvent,\n)\nfrom llama_index.core import (\n    VectorStoreIndex,\n    Document,\n    SummaryIndex,\n)\nfrom llama_index.core.query_pipeline import QueryPipeline\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import StorageContext\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores.pinecone import PineconeVectorStore\nfrom IPython.display import Markdown, display\nfrom llama_index.core.base.base_retriever import BaseRetriever\n\nclass OpenSourceRAG(Workflow):\n    def __init__(self):\n        super().__init__()\n        self.conversation_history = []\n\n    @step\n    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n        \"\"\"Ingest step (for ingesting docs and initializing index).\"\"\"\n        documents: list[Document] | None = ev.get(\"documents\")\n\n        if documents is None:\n            return None\n\n        vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n        index = VectorStoreIndex.from_documents(\n            documents, storage_context=storage_context\n        )\n\n        return StopEvent(result=index)\n\n    @step\n    async def prepare_for_retrieval(\n        self, ctx: Context, ev: StartEvent\n    ) -> PrepEvent | None:\n        \"\"\"Prepare for retrieval.\"\"\"\n\n        model_url = \"https://l26a2czy10oerq1x.us-east-1.aws.endpoints.huggingface.cloud\"\n\n        query_str: str | None = ev.get(\"query_str\")\n        retriever_kwargs: dict | None = ev.get(\"retriever_kwargs\", {})\n\n        if query_str is None:\n            return None\n\n        index = ev.get(\"index\")\n\n        llm = TextGenerationInference(\n            model_url=model_url,\n            token=os.environ[\"HF_TOKEN\"],\n            model_name=\"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4 \"\n        )\n        await ctx.set(\"rag_pipeline\", QueryPipeline(\n            chain=[DEFAULT_RAG_PROMPT, llm]\n        ))\n\n        await ctx.set(\"llm\", llm)\n        await ctx.set(\"index\", index)\n\n        # Add the new query to the conversation history\n        self.conversation_history.append(f\"Human: {query_str}\")\n\n        await ctx.set(\"query_str\", query_str)\n        await ctx.set(\"retriever_kwargs\", retriever_kwargs)\n\n        return PrepEvent()\n\n    @step\n    async def retrieve(\n        self, ctx: Context, ev: PrepEvent\n    ) -> RetrieveEvent | None:\n        \"\"\"Retrieve the relevant nodes for the query.\"\"\"\n        query_str = await ctx.get(\"query_str\")\n        retriever_kwargs = await ctx.get(\"retriever_kwargs\")\n\n        if query_str is None:\n            return None\n\n        index = await ctx.get(\"index\", default=None)\n        if not (index):\n            raise ValueError(\n                \"Index and tavily tool must be constructed. Run with 'documents' and 'tavily_ai_apikey' params first.\"\n            )\n\n        retriever: BaseRetriever = index.as_retriever(\n            **retriever_kwargs\n        )\n        result = retriever.retrieve(query_str)\n        await ctx.set(\"query_str\", query_str)\n        return RetrieveEvent(retrieved_nodes=result)\n\n    @step\n    async def augment_and_generate(self, ctx: Context, ev: RetrieveEvent) -> StopEvent:\n        \"\"\"Get result with relevant text.\"\"\"\n        relevant_nodes = ev.retrieved_nodes\n        relevant_text = \"\\n\".join([node.get_content() for node in relevant_nodes])\n        query_str = await ctx.get(\"query_str\")\n\n        relevancy_pipeline = await ctx.get(\"rag_pipeline\")\n\n        # Prepare the conversation history string\n        conversation_history = \"\\n\".join(self.conversation_history)\n\n        relevancy = relevancy_pipeline.run(\n                conversation_history=conversation_history,\n                context=relevant_text,\n                question=query_str\n        )\n\n        # Add the assistant's response to the conversation history\n        self.conversation_history.append(f\"Assistant: {relevancy.message.content}\")\n        if(len(self.conversation_history) > 2048):\n          self.conversation_history = self.conversation_history[-1800:]\n\n        return StopEvent(result=relevancy.message.content)","metadata":{"id":"vf4zPnR2LdL8","execution":{"iopub.status.busy":"2024-10-13T18:25:22.708938Z","iopub.execute_input":"2024-10-13T18:25:22.709635Z","iopub.status.idle":"2024-10-13T18:25:23.797406Z","shell.execute_reply.started":"2024-10-13T18:25:22.709581Z","shell.execute_reply":"2024-10-13T18:25:23.796369Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## \"Graphing\" our Workflow\n\nSince we have `Steps` that take `Events` and return `Events` - we can trace through all possible paths and wind up with a graph!","metadata":{"id":"zFnSHlovYTZa"}},{"cell_type":"code","source":"from llama_index.utils.workflow import draw_all_possible_flows\n\ndraw_all_possible_flows(\n    OpenSourceRAG, filename=\"os_rag_workflow.html\"\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r7mBeY0vhOww","outputId":"e7af0fb5-e5fc-44f0-c1dd-3378c1ae336a","execution":{"iopub.status.busy":"2024-10-13T18:25:24.964537Z","iopub.execute_input":"2024-10-13T18:25:24.965445Z","iopub.status.idle":"2024-10-13T18:25:25.081717Z","shell.execute_reply.started":"2024-10-13T18:25:24.965395Z","shell.execute_reply":"2024-10-13T18:25:25.080505Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"os_rag_workflow.html\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Using our Workflow\n\nFirst, we need to set-up our documents, then initialize our Index!","metadata":{"id":"7PaSuse9YVbs"}},{"cell_type":"code","source":"all_documents = doc_list","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:25:27.740402Z","iopub.execute_input":"2024-10-13T18:25:27.741192Z","iopub.status.idle":"2024-10-13T18:25:27.747136Z","shell.execute_reply.started":"2024-10-13T18:25:27.741141Z","shell.execute_reply":"2024-10-13T18:25:27.745636Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from llama_index.core import SimpleDirectoryReader\n\nrag_workflow = OpenSourceRAG()\nindex = await rag_workflow.run(documents=all_documents)","metadata":{"id":"NnvyideuhQ3f","execution":{"iopub.status.busy":"2024-10-13T18:26:17.182344Z","iopub.execute_input":"2024-10-13T18:26:17.183502Z","iopub.status.idle":"2024-10-13T18:33:42.184356Z","shell.execute_reply.started":"2024-10-13T18:26:17.183452Z","shell.execute_reply":"2024-10-13T18:33:42.182928Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Upserted vectors:   0%|          | 0/2048 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a0341455e0249cea5b278f203379d98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upserted vectors:   0%|          | 0/2048 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"875c834e917f4f46bb6ad7feabd07d56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upserted vectors:   0%|          | 0/874 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06a4ee6ed26d4220b7e6b6ba6ffd883f"}},"metadata":{}}]},{"cell_type":"markdown","source":"Now we're ready to query our Workflow!","metadata":{"id":"sV06vzDdYavQ"}},{"cell_type":"code","source":"from IPython.display import Markdown, display\n\nresponse = await rag_workflow.run(\n    query_str=\"Why did Elon Musk sue OpenAI?\",\n    index=index,\n)\ndisplay(Markdown(str(response)))","metadata":{"id":"PjqbTHsMjk1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Markdown, display\n\nresponse = await rag_workflow.run(\n    # query_str=\"Give me some cases based on discrimination by sex\",\n    query_str= \"What happened in the LEXSEE 2003 U.S. DIST. CT. PLEADINGS 3030\",\n    index=index,\n)\ndisplay(Markdown(str(response)))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136},"id":"YbnSXobGkmF4","outputId":"daf73ece-5f7e-455c-ed89-a9a5314d3879","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding evaluation metrics","metadata":{}},{"cell_type":"code","source":"!pip install -q \"arize-phoenix>=4.29.0\"\n!pip install -q openai\n!pip install -q openinference-instrumentation-openai","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nif \"PHOENIX_API_KEY\" in os.environ:\n    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=5ce09c685e4468536bd:dd1fd34\"\n    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n    print(\"Using cloud instance of Phoenix.\")\nelse:\n    import phoenix as px\n    px.launch_app().view()\n    print(\"Using local instance of Phoenix.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from phoenix.otel import register\n\ntracer_provider = register(project_name=\"ragathon-2024\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from openinference.instrumentation.openai import OpenAIInstrumentor\n\nOpenAIInstrumentor().instrument(tracer_provider=tracer_provider)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from openai import OpenAI\n\n# Initialize OpenAI client\nclient = OpenAI(model=\"gpt-4o\",temperature=0.4)\n\n\n# Function to generate a case\ndef generate():\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that helps user with their legal issues. User asks you questions they have about their situation and you answer based on the context you have and if you find anything on the internet.\"},\n            {\"role\": \"user\", \"content\": \"Tell me a situation where I an avoid being kicked out of my apartment in California.\"},\n        ],\n    )\n    case = response.choices[0].message.content\n    return case\n\n\n# Generate 5 different jokes\ncases = []\nfor _ in range(5):\n    case = generate()\n    cases.append(case)\n    print(f\"Joke {len(cases)}:\\n{case}\\n\")\n\nprint(f\"Generated {len(cases)} jokes and tracked them in Phoenix.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import phoenix as px\n\nspans_df = px.Client().get_spans_dataframe(project_name=\"ragathon-2024\")\nspans_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new DataFrame with selected columns\neval_df = spans_df[[\"context.span_id\", \"attributes.llm.output_messages\"]].copy()\neval_df.set_index(\"context.span_id\", inplace=True)\n\n# Create a list to store unique jokes\nunique_jokes = set()\n\n\n# Function to check if a joke is a duplicate\ndef is_duplicate(joke_data):\n    joke = joke_data[0][\"message.content\"]\n    if joke in unique_jokes:\n        return True\n    else:\n        unique_jokes.add(joke)\n        return False\n\n\n# Apply the is_duplicate function to create the new column\neval_df[\"label\"] = eval_df[\"attributes.llm.output_messages\"].apply(is_duplicate)\n\n# Convert boolean to integer (0 for False, 1 for True)\neval_df[\"label\"] = eval_df[\"label\"]\n\n# Reset unique_jokes list to ensure correct results if the cell is run multiple times\nunique_jokes.clear()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_df[\"score\"] = eval_df[\"label\"].astype(int)\neval_df[\"label\"] = eval_df[\"label\"].astype(str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from phoenix.trace import SpanEvaluations\n\npx.Client().log_evaluations(SpanEvaluations(eval_name=\"Duplicate\", dataframe=eval_df))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multiagent implementation","metadata":{}},{"cell_type":"code","source":"!pip install  llama_index\n!pip install llama-index-tools-tavily-research\n!pip install colorama","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DFA8gSRpm7Nc","outputId":"581fe23e-5680-408f-a56c-da5bd41c7bc1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.embeddings.mistralai import MistralAIEmbedding\nfrom dotenv import load_dotenv\nimport asyncio\nfrom llama_index.core.workflow import Workflow, StartEvent, StopEvent, step\nimport os\nfrom typing import List, Optional\nfrom llama_index.llms.text_generation_inference import (\n    TextGenerationInference,\n)\nfrom llama_index.core import PromptTemplate\nfrom llama_index.core import Settings\nfrom llama_index.vector_stores.pinecone import PineconeVectorStore\nfrom pinecone import Pinecone, ServerlessSpec\nfrom llama_index.core.workflow import Event\nfrom llama_index.core.schema import NodeWithScore\nfrom llama_index.core.workflow import (\n    Workflow,\n    step,\n    Context,\n    StartEvent,\n    StopEvent,\n)\nfrom llama_index.core import (\n    VectorStoreIndex,\n    Document,\n    SummaryIndex,\n)\nfrom llama_index.core.query_pipeline import QueryPipeline\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import StorageContext\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores.pinecone import PineconeVectorStore\nfrom IPython.display import Markdown, display\nfrom llama_index.core.base.base_retriever import BaseRetriever","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"load_dotenv()\n\nSettings.llm = OpenAI(model=\"gpt-4o\", temperature=0.1)\n\n### --- INITS --- ###\n\npc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n\nembed_model = MistralAIEmbedding(model_name=\"mistral-embed\")\nSettings.embed_model = embed_model\n\nindex_name = \"llamaindex-ragathon-lawllm-long-index-v1\"\npinecone_index = pc.Index(index_name)\nvector_store = PineconeVectorStore(\n    pinecone_index=pinecone_index,\n)\nstorage_context = StorageContext.from_defaults(\n  vector_store=vector_store\n)\nvector_store_index = VectorStoreIndex.from_vector_store(vector_store)\n\nDEFAULT_RAG_PROMPT = PromptTemplate(\n    template=\"\"\"Use the provided context to answer the question. If you don't know the answer, say you don't know.\n\n    Context:\n    {context}\n\n    Question:\n    {question}\n    \"\"\"\n)\n\n### ---!! RAG QueryEngine Tool !!--- ###\n\nqa_prompt = PromptTemplate(\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query.\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\n\nclass RAGStringQueryEngine(CustomQueryEngine):\n    \"\"\"RAG String Query Engine.\"\"\"\n\n    retriever: BaseRetriever\n    response_synthesizer: BaseSynthesizer\n    llm: TextGenerationInference\n    qa_prompt: PromptTemplate\n\n    def custom_query(self, query_str: str):\n        nodes = self.retriever.retrieve(query_str)\n\n        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n        response = self.llm.complete(\n            qa_prompt.format(context_str=context_str, query_str=query_str)\n        )\n\n        return str(response)\n\npincecone_retriever = vector_store_index.as_retriever()\nsynthesizer = get_response_synthesizer(response_mode=\"compact\")\n\n\n\nhf_llm = TextGenerationInference(\n    model_name=\"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\",\n    model_url= \"https://l26a2czy10oerq1x.us-east-1.aws.endpoints.huggingface.cloud\",\n    token=os.environ[\"HF_TOKEN\"]\n)\n\nrag_query_engine = RAGStringQueryEngine(\n    retriever=pincecone_retriever,\n    response_synthesizer=synthesizer,\n    llm=hf_llm,\n    qa_prompt=qa_prompt,\n)\n\nrag_tool = QueryEngineTool.from_defaults(\n    rag_query_engine, name=\"ragtool\", description=\"Useful for when you want to more about legal cases\"\n)\n\n### ---!! MULTI-AGENT WORKFLOW !!--- ###\n\n### --- EVENTS --- ####\n\nclass InitializeEvent(Event):\n    pass\n\nclass InterfaceAgentEvent(Event):\n    request: Optional[str] = None\n    just_completed: Optional[str] = None\n    need_help: Optional[bool] = None\n\nclass OrchestratorEvent(Event):\n    request: str\n\nclass WebSearchEvent(Event):\n    request: str\n\nclass RAGSearchEvent(Event):\n    request: str\n\n### --- WORKFLOW --- ###\n\nclass MultiAgentRag(Workflow):\n    @step(pass_context=True)\n    async def initialize(self, ctx: Context, ev: InitializeEvent) -> InterfaceAgentEvent:\n        ctx.data[\"initialized\"] = None\n        ctx.data[\"success\"] = None\n        ctx.data[\"redirecting\"] = None\n        ctx.data[\"overall_request\"] = None\n\n        ctx.data[\"llm\"] = OpenAI(model=\"gpt-4o\",temperature=0.4)\n\n        return InterfaceAgentEvent()\n\n    @step(pass_context=True)\n    async def interface_agent(self, ctx: Context, ev: InterfaceAgentEvent | StartEvent) -> InitializeEvent | StopEvent | OrchestratorEvent:\n        if (\"initialized\" not in ctx.data):\n            return InitializeEvent()\n\n        if (\"InterfaceAgent\" not in ctx.data):\n            system_prompt = (f\"\"\"\n                You are a helpful assistant that is helping a user to get some legal advice.\n                Your job is to ask the user questions to figure out what they want to do, and give them the similar legal cases.\n                That includes\n                1. Searching legal cases from the database\n                2. Suggesting attorneys through the web\n                You should start by listing the things you can help them do.\n            \"\"\")\n\n            agent_worker = FunctionCallingAgentWorker.from_tools(\n                tools=[],\n                llm=ctx.data[\"llm\"],\n                allow_parallel_tool_calls=False,\n                system_prompt=system_prompt\n            )\n            ctx.data[\"InterfaceAgent\"] = agent_worker.as_agent()\n\n        interface_agent = ctx.data[\"InterfaceAgent\"]\n        if ctx.data[\"overall_request\"] is not None:\n            print(\"There's an overall request in progress, it's \", ctx.data[\"overall_request\"])\n            last_request = ctx.data[\"overall_request\"]\n            ctx.data[\"overall_request\"] = None\n            return OrchestratorEvent(request=last_request)\n        elif (ev.just_completed is not None):\n            response = interface_agent.chat(f\"FYI, the user has just completed the task: {ev.just_completed}\")\n        elif (ev.need_help):\n            print(\"The previous process needs help with \", ev.request)\n            return OrchestratorEvent(request=ev.request)\n        else:\n            response = interface_agent.chat(\"Hello!\")\n\n        print(Fore.MAGENTA + str(response) + Style.RESET_ALL)\n        user_msg_str = input(\"> \").strip()\n        return OrchestratorEvent(request=user_msg_str)\n\n    @step(pass_context=True)\n    async def orchestrator(self, ctx: Context, ev: OrchestratorEvent) -> InterfaceAgentEvent | WebSearchEvent | RAGSearchEvent | StopEvent:\n\n        print(f\"Orchestrator received request: {ev.request}\")\n\n        def emit_web_lookup() -> bool:\n            \"\"\"Call this if the user wants to look up information about latest legal cases\"\"\"\n            print(\"__emitted: WebSearchEvent\")\n            self.send_event(WebSearchEvent(request=ev.request))\n            return True\n\n        def emit_rag_lookup() -> bool:\n            \"\"\"Call this if the user wants to get similar legal cases and laws used in the past\"\"\"\n            print(\"__emitted: RAGSearchEvent\")\n            self.send_event(RAGSearchEvent(request=ev.request))\n            return True\n\n        def emit_interface_agent() -> bool:\n            \"\"\"Call this if the user wants to do something else or you can't figure out what they want to do.\"\"\"\n            print(\"__emitted: interface\")\n            self.send_event(InterfaceAgentEvent(request=ev.request))\n            return True\n\n        def emit_stop() -> bool:\n            \"\"\"Call this if the user wants to stop or exit the system.\"\"\"\n            print(\"__emitted: stop\")\n            self.send_event(StopEvent())\n            return True\n\n        tools = [\n            FunctionTool.from_defaults(fn=emit_web_lookup),\n            FunctionTool.from_defaults(fn=emit_rag_lookup),\n            FunctionTool.from_defaults(fn=emit_interface_agent),\n            FunctionTool.from_defaults(fn=emit_stop)\n        ]\n\n        system_prompt = (f\"\"\"\n            You are on orchestration agent.\n            Your job is to decide which agent to run based on the current state of the user and what they've asked to do.\n            You run an agent by calling the appropriate tool for that agent.\n            You do not need to call more than one tool.\n            You do not need to figure out dependencies between agents; the agents will handle that themselves.\n\n            If you did not call any tools, return the string \"FAILED\" without quotes and nothing else.\n        \"\"\")\n\n        agent_worker = FunctionCallingAgentWorker.from_tools(\n            tools=tools,\n            llm=ctx.data[\"llm\"],\n            allow_parallel_tool_calls=False,\n            system_prompt=system_prompt\n        )\n        ctx.data[\"orchestrator\"] = agent_worker.as_agent()\n\n        orchestrator = ctx.data[\"orchestrator\"]\n        response = str(orchestrator.chat(ev.request))\n\n        if response == \"FAILED\":\n            print(\"Orchestration agent failed to return a valid speaker; try again\")\n            return OrchestratorEvent(request=ev.request)\n\n    @step(pass_context=True)\n    async def web_search(self, ctx: Context, ev: WebSearchEvent) -> InterfaceAgentEvent:\n\n        print(f\"WebSearch received request: {ev.request}\")\n\n        if (\"web_search_agent\" not in ctx.data):\n            taviliy_tool = TavilyToolSpec(api_key=os.environ[\"TAVILY_API_KEY\"])\n\n\n            system_prompt = (f\"\"\"\n                You are a helpful assistant that is helping a user get legal advice.\n                Once you have retrieved information about the previous cases, you *must* call the tool named \"done\" to signal that you are done. Do this before you respond.\n                If the user asks to do anything other than look up information on new cases, call the tool \"need_help\" to signal some other agent should help.\n            \"\"\")\n\n            ctx.data[\"web_search_agent\"] = BaseAgent(\n                name=\"Web Search Agent\",\n                parent=self,\n                tools=taviliy_tool.to_tool_list(),\n                context=ctx,\n                system_prompt=system_prompt,\n                trigger_event=WebSearchEvent\n            )\n\n        return ctx.data[\"web_search_agent\"].handle_event(ev)\n\n    @step(pass_context=True)\n    async def rag_lookup(self, ctx: Context, ev: RAGSearchEvent) -> InterfaceAgentEvent:\n\n        print(f\"RAG received request: {ev.request}\")\n\n        if (\"rag_search_agent\" not in ctx.data):\n            rag_tool = QueryEngineTool.from_defaults(\n                rag_query_engine, name=\"ragtool\", description=\"Useful when you want to know more about legal cases\"\n            )\n\n\n            system_prompt = (f\"\"\"\n                You are a helpful assistant that is that is helping a user get legal advice.\n                Once you have retrieved information about the complaint/lawsuit, you *must* call the tool named \"done\" to signal that you are done. Do this before you respond.\n                If the user asks to do anything other than look up information on legal cases, call the tool \"need_help\" to signal some other agent should help.\n            \"\"\")\n\n            ctx.data[\"rag_search_agent\"] = BaseAgent(\n                name=\"RAG Search Agent\",\n                parent=self,\n                tools=[rag_tool],\n                context=ctx,\n                system_prompt=system_prompt,\n                trigger_event=RAGSearchEvent\n            )\n\n        return ctx.data[\"rag_search_agent\"].handle_event(ev)\n\nclass BaseAgent():\n    name: str\n    parent: Workflow\n    tools: list[FunctionTool]\n    system_prompt: str\n    context: Context\n    current_event: Event\n    trigger_event: Event\n\n    def __init__(\n            self,\n            parent: Workflow,\n            tools: List[Callable],\n            system_prompt: str,\n            trigger_event: Event,\n            context: Context,\n            name: str,\n        ):\n        self.name = name\n        self.parent = parent\n        self.context = context\n        self.system_prompt = system_prompt\n        self.context.data[\"redirecting\"] = False\n        self.trigger_event = trigger_event\n\n        # set up the tools including the ones everybody gets\n        def done() -> None:\n            \"\"\"When you complete your task, call this tool.\"\"\"\n            print(f\"{self.name} is complete\")\n            self.context.data[\"redirecting\"] = True\n            parent.send_event(InterfaceAgentEvent(just_completed=self.name))\n\n        def need_help() -> None:\n            \"\"\"If the user asks to do something you don't know how to do, call this.\"\"\"\n            print(f\"{self.name} needs help\")\n            self.context.data[\"redirecting\"] = True\n            parent.send_event(InterfaceAgentEvent(request=self.current_event.request,need_help=True))\n\n        self.tools = [\n            FunctionTool.from_defaults(fn=done),\n            FunctionTool.from_defaults(fn=need_help)\n        ]\n        for t in tools:\n            self.tools.append(t)\n\n        agent_worker = FunctionCallingAgentWorker.from_tools(\n            self.tools,\n            llm=self.context.data[\"llm\"],\n            allow_parallel_tool_calls=False,\n            system_prompt=self.system_prompt\n        )\n        self.agent = agent_worker.as_agent()\n\n    def handle_event(self, ev: Event):\n        self.current_event = ev\n\n        response = str(self.agent.chat(ev.request))\n        print(Fore.MAGENTA + str(response) + Style.RESET_ALL)\n\n        # if they're sending us elsewhere we're done here\n        if self.context.data[\"redirecting\"]:\n            self.context.data[\"redirecting\"] = False\n            return None\n\n        # otherwise, get some user input and then loop\n        user_msg_str = input(\"> \").strip()\n        return self.trigger_event(request=user_msg_str)\n\n\ndraw_all_possible_flows(MultiAgentRag,filename=\"multi-agent-rag.html\")\n\nasync def main():\n    c = MultiAgentRag(timeout=1200, verbose=True)\n    result = await c.run()\n    print(result)\n\nawait main()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{"id":"nGWrP21olzpB"}}]}